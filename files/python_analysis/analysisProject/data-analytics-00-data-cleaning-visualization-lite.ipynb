{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a64d2c",
   "metadata": {},
   "source": [
    "# Data Cleaning and Visualization - Lite Version\n",
    "\n",
    "This notebook contains all the Python code from the data cleaning slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da26240",
   "metadata": {},
   "source": [
    "## Step 0: Load Python's Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f87d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Import Required Libraries\n",
    "# ========================================\n",
    "# pandas (pd): The primary library for working with tabular data (like spreadsheets)\n",
    "import pandas as pd\n",
    "\n",
    "# numpy (np): Library for numerical computations and array operations\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib.pyplot (plt): The main plotting library for creating visualizations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f56a4",
   "metadata": {},
   "source": [
    "## Step 1 (a): Load Your Data From a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# First look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b12af",
   "metadata": {},
   "source": [
    "## Step 1 (b): Prepare (Random) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58688a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime modules: For working with dates and times\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ========================================\n",
    "# Configure Settings\n",
    "# ========================================\n",
    "# Set random seed to 42 so everyone gets the same \"random\" data\n",
    "# This makes our code reproducible - you'll get the same results every time\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure pandas display options for better readability\n",
    "pd.set_option('display.max_columns', None)  # Show all columns (don't truncate)\n",
    "pd.set_option('display.width', None)        # Use full screen width\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")\n",
    "print(\"✓ Settings configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe04238",
   "metadata": {},
   "source": [
    "## Step 1: Create The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Create Sample Dataset with Intentional Data Quality Issues\n",
    "# ========================================\n",
    "# We're creating a realistic dataset that has common problems you'll encounter in real data\n",
    "\n",
    "# Set the number of records we want to generate\n",
    "n_records = 500\n",
    "\n",
    "# ----------------------------------------\n",
    "# Generate Random Dates\n",
    "# ----------------------------------------\n",
    "# Create a starting date (January 1, 2024)\n",
    "start_date = datetime(2024, 1, 1)\n",
    "\n",
    "# Generate 500 random dates throughout 2024\n",
    "# This simulates transaction dates spread across the year\n",
    "dates = [start_date + timedelta(days=np.random.randint(0, 365)) for _ in range(n_records)]\n",
    "\n",
    "# ----------------------------------------\n",
    "# Build the Dataset Dictionary\n",
    "# ----------------------------------------\n",
    "# Create a dictionary where each key is a column name and value is a list of data\n",
    "data = {\n",
    "    # Transaction date for each sale\n",
    "    'date': dates,\n",
    "    \n",
    "    # Product names - randomly chosen from 6 different products\n",
    "    'product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse'], n_records),\n",
    "    \n",
    "    # Region names - NOTE: Intentionally inconsistent capitalization ('north' vs 'North' vs 'SOUTH')\n",
    "    # This is a common data quality issue we'll need to fix!\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West', 'north', 'SOUTH'], n_records),\n",
    "    \n",
    "    # Sales amount in dollars - random values between $100 and $5,000\n",
    "    'sales': np.random.randint(100, 5000, n_records),\n",
    "    \n",
    "    # Quantity of items sold - random values between 1 and 50\n",
    "    'quantity': np.random.randint(1, 50, n_records),\n",
    "    \n",
    "    # Customer age - random values between 18 and 75\n",
    "    'customer_age': np.random.randint(18, 75, n_records),\n",
    "    \n",
    "    # Customer satisfaction score (1-5 scale, where 5 is best)\n",
    "    'satisfaction': np.random.choice([1, 2, 3, 4, 5], n_records)\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a pandas DataFrame (like a spreadsheet table)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Introduce Missing Values (10% of data)\n",
    "# ----------------------------------------\n",
    "# Randomly select 10% of rows to have missing data\n",
    "missing_indices = np.random.choice(df.index, size=int(n_records * 0.10), replace=False)\n",
    "\n",
    "# Make half of those rows have missing sales values\n",
    "df.loc[missing_indices[:len(missing_indices)//2], 'sales'] = np.nan\n",
    "\n",
    "# Make the other half have missing satisfaction scores\n",
    "df.loc[missing_indices[len(missing_indices)//2:], 'satisfaction'] = np.nan\n",
    "\n",
    "# ----------------------------------------\n",
    "# Introduce Duplicate Rows\n",
    "# ----------------------------------------\n",
    "# Randomly select 20 rows and duplicate them\n",
    "duplicate_rows = df.sample(20)\n",
    "# Add these duplicate rows to the dataframe (this creates duplicates)\n",
    "df = pd.concat([df, duplicate_rows], ignore_index=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Introduce Outliers\n",
    "# ----------------------------------------\n",
    "# Select 10 random rows and give them unrealistically high sales values\n",
    "outlier_indices = np.random.choice(df.index, size=10, replace=False)\n",
    "# Set their sales to be between $50,000 and $100,000 (much higher than normal)\n",
    "df.loc[outlier_indices, 'sales'] = np.random.randint(50000, 100000, len(outlier_indices))\n",
    "\n",
    "# ----------------------------------------\n",
    "# Display Results\n",
    "# ----------------------------------------\n",
    "print(f\"✓ Dataset created with {len(df)} records\")\n",
    "print(f\"✓ Includes: missing values, duplicates, outliers, and inconsistent data\")\n",
    "print(f\"\\nFirst 10 rows of the dataset:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37080dc",
   "metadata": {},
   "source": [
    "## Step 2: Check Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ab8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information\n",
    "df.info()\n",
    "\n",
    "# Check dimensions\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "\n",
    "# View column names and types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647de55d",
   "metadata": {},
   "source": [
    "## Step 3: Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a82965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values\n",
    "missing = df.isnull().sum()\n",
    "\n",
    "# Calculate percentage missing\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Display summary\n",
    "pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1fdd79",
   "metadata": {},
   "source": [
    "## Step 3: Checking for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4565df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicate rows\n",
    "print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "\n",
    "# View duplicate rows\n",
    "df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141fe87e",
   "metadata": {},
   "source": [
    "## Step 3: Checking for Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2428e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at unique values in categorical columns\n",
    "df['region'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2c4f0",
   "metadata": {},
   "source": [
    "# Part 2: Data Cleaning\n",
    "\n",
    "## Step 1: Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy (preserve original!)\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Remove duplicates\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "print(f\"Removed {len(df) - len(df_clean)} duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5ce1b",
   "metadata": {},
   "source": [
    "## Step 2: Standardize Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42933d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix inconsistent capitalization\n",
    "df_clean['region'] = df_clean['region'].str.title()\n",
    "\n",
    "# Before: 'north', 'NORTH', 'North'\n",
    "# After:  'North', 'North', 'North'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdbcc8a",
   "metadata": {},
   "source": [
    "## Step 3: Filling Missing Values - Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing sales with median by product\n",
    "df_clean['sales'] = df_clean.groupby('product')['sales'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf01c09",
   "metadata": {},
   "source": [
    "## Step 3: Filling Missing Values - Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee54728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill satisfaction scores with mode\n",
    "mode_value = df_clean['satisfaction'].mode()[0]\n",
    "df_clean['satisfaction'] = df_clean['satisfaction'].fillna(mode_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b1168a",
   "metadata": {},
   "source": [
    "## Step 4: Identify Outliers (IQR Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df_clean['sales'].quantile(0.25)\n",
    "Q3 = df_clean['sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Find outliers\n",
    "outliers = df_clean[\n",
    "    (df_clean['sales'] < lower_bound) | \n",
    "    (df_clean['sales'] > upper_bound)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d108f58a",
   "metadata": {},
   "source": [
    "## Step 5: Handle Outliers - Option 1 (Remove Them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[\n",
    "    (df_clean['sales'] >= lower_bound) & \n",
    "    (df_clean['sales'] <= upper_bound)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b515cc",
   "metadata": {},
   "source": [
    "## Step 5: Handle Outliers - Option 2 (Cap Them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['sales'] = df_clean['sales'].clip(\n",
    "    lower=lower_bound, \n",
    "    upper=upper_bound\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9b725",
   "metadata": {},
   "source": [
    "## Step 6: Add Derived Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac63a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate price per unit\n",
    "df_clean['price_per_unit'] = df_clean['sales'] / df_clean['quantity']\n",
    "\n",
    "# Extract time features\n",
    "df_clean['month'] = pd.to_datetime(df_clean['date']).dt.month\n",
    "df_clean['quarter'] = pd.to_datetime(df_clean['date']).dt.quarter\n",
    "\n",
    "# Create age groups\n",
    "df_clean['age_group'] = pd.cut(\n",
    "    df_clean['customer_age'], \n",
    "    bins=[0, 25, 35, 50, 100],\n",
    "    labels=['18-25', '26-35', '36-50', '50+']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ec867",
   "metadata": {},
   "source": [
    "# Part 3: Data Subsetting\n",
    "\n",
    "## Filtering by Single Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by product\n",
    "laptops = df_clean[df_clean['product'] == 'Laptop']\n",
    "\n",
    "# Filter by value range\n",
    "high_sales = df_clean[df_clean['sales'] > 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab402ab4",
   "metadata": {},
   "source": [
    "## Filtering by Multiple Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND condition: both must be true\n",
    "premium = df_clean[\n",
    "    (df_clean['satisfaction'] >= 4) & \n",
    "    (df_clean['sales'] > 2000)\n",
    "]\n",
    "\n",
    "# OR condition: either can be true\n",
    "tech_products = df_clean[\n",
    "    (df_clean['product'] == 'Laptop') | \n",
    "    (df_clean['product'] == 'Phone')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce01b68",
   "metadata": {},
   "source": [
    "## Using .isin() for Multiple Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4789381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for multiple categories\n",
    "high_value = df_clean[\n",
    "    df_clean['product'].isin(['Laptop', 'Monitor'])\n",
    "]\n",
    "\n",
    "# Filter by multiple regions and quarters\n",
    "subset = df_clean[\n",
    "    (df_clean['quarter'].isin([1, 2])) & \n",
    "    (df_clean['region'].isin(['North', 'East']))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd44bd",
   "metadata": {},
   "source": [
    "# Best Practices\n",
    "\n",
    "## Validate Your Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48751baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for remaining issues\n",
    "assert df_clean.duplicated().sum() == 0\n",
    "assert df_clean.isnull().sum().sum() == 0\n",
    "\n",
    "# Verify data makes sense\n",
    "print(f\"Sales range: ${df_clean['sales'].min()} - ${df_clean['sales'].max()}\")\n",
    "print(f\"Unique regions: {df_clean['region'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce5f0a",
   "metadata": {},
   "source": [
    "## Document Your Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675b0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document your cleaning steps\n",
    "cleaning_log = {\n",
    "    'duplicates_removed': 20,\n",
    "    'missing_sales_filled': 'median by product',\n",
    "    'missing_satisfaction_filled': 'mode',\n",
    "    'outliers_removed': 10,\n",
    "    'reason': 'Values exceeded 3x IQR'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f26a86",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "## Visualizing Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbbc121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Missing values heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(df.isnull(), cmap='viridis', aspect='auto')\n",
    "plt.title('Missing Values Pattern')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddd4f6",
   "metadata": {},
   "source": [
    "## Before/After Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a5a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before\n",
    "df['sales'].hist(ax=ax1, bins=30)\n",
    "ax1.set_title('Sales Distribution - Before Cleaning')\n",
    "\n",
    "# After\n",
    "df_clean['sales'].hist(ax=ax2, bins=30)\n",
    "ax2.set_title('Sales Distribution - After Cleaning')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1bda14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've completed the data cleaning workflow! Your data is now:\n",
    "- Free of duplicates\n",
    "- Standardized\n",
    "- Free of missing values\n",
    "- Outliers handled\n",
    "- Enhanced with derived features\n",
    "\n",
    "**Remember:** Clean data = Quality analysis = Better insights"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
